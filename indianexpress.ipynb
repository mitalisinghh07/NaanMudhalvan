# Install necessary packages
!pip install nltk spacy scikit-learn xgboost joblib seaborn --quiet

# Import libraries and download required datasets
import nltk
import spacy
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from google.colab import files
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from nltk.corpus import stopwords

# Download NLTK datasets
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Download SpaCy model
!python -m spacy download en_core_web_sm

# Initialize SpaCy and NLTK stopwords
nlp = spacy.load("en_core_web_sm")
stop_words = set(stopwords.words('english'))

# Function to preprocess text: Lowercase, remove stopwords, and lemmatize
def preprocess(text):
    doc = nlp(text.lower())
    return " ".join([token.lemma_ for token in doc if token.is_alpha and token.text not in stop_words])

# Function to load and preprocess the data
def load_and_preprocess_data(file_name):
    # Upload the dataset
    uploaded = files.upload()
    df = pd.read_csv(file_name, encoding='utf-8')

    # Combine the text columns into one
    df['combined_text'] = df['headlines'].fillna('') + ' ' + df['description'].fillna('') + ' ' + df['content'].fillna('')

    # Apply preprocessing
    df['cleaned_text'] = df['combined_text'].apply(preprocess)

    # Drop rows with missing categories
    df = df.dropna(subset=['category']).reset_index(drop=True)

    return df

# Function to filter categories with more than one sample
def filter_categories(df):
    y = df['category']
    unique_categories = y.value_counts()
    filtered_categories = unique_categories[unique_categories > 1].index

    df_filtered = df[df['category'].isin(filtered_categories)].reset_index(drop=True)
    return df_filtered

# Function to convert text to TF-IDF features
def get_tfidf_features(df):
    tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))
    X = tfidf.fit_transform(df['cleaned_text'])
    y = df['category']
    return X, y, tfidf

# Function to split data into training and testing sets
def split_data(X, y):
    return train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Function to train and evaluate a model
def train_and_evaluate_model(X_train, X_test, y_train, y_test):
    # Train logistic regression model
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Evaluate the model
    print("Classification Report:")
    print(classification_report(y_test, y_pred))
    print("Accuracy:", accuracy_score(y_test, y_pred))

    # Plot confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', xticklabels=model.classes_, yticklabels=model.classes_, cmap='Blues')
    plt.title("Confusion Matrix")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

    return model, y_pred

# Function to export predictions
def export_predictions(df, y_test, y_pred, file_name="predictions.csv"):
    df_output = df.iloc[y_test.index.to_list()].copy()
    df_output['Predicted_Category'] = y_pred
    df_output[['headlines', 'Predicted_Category']].to_csv(file_name, index=False)
    print(f"âœ… Predictions saved to {file_name}")

# Main workflow
def main(file_name='indian_express_news.csv'):
    # Step 1: Load and preprocess the data
    df = load_and_preprocess_data(file_name)

    # Step 2: Filter categories with more than one sample
    df_filtered = filter_categories(df)

    # Step 3: Convert text to TF-IDF features
    X, y, tfidf = get_tfidf_features(df_filtered)

    # Step 4: Split data into training and testing sets
    X_train, X_test, y_train, y_test = split_data(X, y)

    # Step 5: Train and evaluate the model
    model, y_pred = train_and_evaluate_model(X_train, X_test, y_train, y_test)

    # Step 6: Export predictions to CSV
    export_predictions(df_filtered, y_test, y_pred)

# Run the main workflow
main()

# Install necessary libraries
!pip install nltk spacy wordcloud scikit-learn matplotlib seaborn plotly

import pandas as pd
import re
import nltk
import seaborn as sns
import matplotlib.pyplot as plt
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from wordcloud import WordCloud
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix

# Download necessary NLTK datasets
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

# Function for text preprocessing
def preprocess_text(text, stop_words, lemmatizer):
    """
    Preprocess the input text by converting it to lowercase, removing non-alphabetic characters,
    tokenizing, and lemmatizing.
    """
    text = str(text).lower()  # Convert text to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and digits
    tokens = word_tokenize(text)  # Tokenize the text
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Lemmatize and remove stopwords
    return ' '.join(tokens)

# Function to label sentiment based on the rating
def label_sentiment(rating):
    """
    Label the sentiment based on the rating:
    Negative: 1 or 2
    Neutral: 3
    Positive: 4 or 5
    """
    if rating <= 2:
        return 'Negative'
    elif rating == 3:
        return 'Neutral'
    else:
        return 'Positive'

# Function to load and preprocess the dataset
def load_and_preprocess_data(url):
    """
    Load the dataset from the URL and apply preprocessing steps (cleaning and sentiment labeling).
    """
    df = pd.read_csv(url)
    
    # Initialize stopwords and lemmatizer
    stop_words = set(stopwords.words('english'))
    lemmatizer = WordNetLemmatizer()

    # Preprocess the review text
    df['clean_review'] = df['review'].apply(preprocess_text, args=(stop_words, lemmatizer))
    
    # Drop rows with missing values in 'clean_review'
    df.dropna(subset=['clean_review'], inplace=True)

    # Apply sentiment labeling
    df['sentiment'] = df['rating'].apply(label_sentiment)
    
    return df

# Function to visualize sentiment distribution
def plot_sentiment_distribution(df):
    """
    Plot the sentiment distribution of the dataset.
    """
    sns.countplot(data=df, x='sentiment', palette='Set2')
    plt.title("Sentiment Distribution")
    plt.show()

# Function to create and display word clouds
def generate_wordclouds(df):
    """
    Generate and display word clouds for positive and negative reviews.
    """
    positive_text = ' '.join(df[df['sentiment'] == 'Positive']['clean_review'])
    negative_text = ' '.join(df[df['sentiment'] == 'Negative']['clean_review'])

    # Generate word clouds
    wordcloud_pos = WordCloud(width=800, height=400, background_color='white').generate(positive_text)
    wordcloud_neg = WordCloud(width=800, height=400, background_color='white').generate(negative_text)

    # Plot word clouds
    plt.figure(figsize=(15, 7))
    plt.subplot(1, 2, 1)
    plt.imshow(wordcloud_pos, interpolation='bilinear')
    plt.title("Positive Reviews WordCloud")
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(wordcloud_neg, interpolation='bilinear')
    plt.title("Negative Reviews WordCloud")
    plt.axis('off')
    plt.show()

# Function to train and evaluate the model
def train_and_evaluate_model(X_train, X_test, y_train, y_test):
    """
    Train a logistic regression model and evaluate its performance using classification report and confusion matrix.
    """
    model = LogisticRegression(max_iter=1000)
    model.fit(X_train, y_train)

    # Make predictions
    y_pred = model.predict(X_test)

    # Print classification report and confusion matrix
    print("Classification Report:\n")
    print(classification_report(y_test, y_pred))
    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))
    
    return model, y_pred

# Function to create the TF-IDF features and train-test split
def create_features_and_split(df):
    """
    Convert the cleaned reviews into TF-IDF features and split the data into train and test sets.
    """
    vectorizer = TfidfVectorizer(max_features=5000)
    X = vectorizer.fit_transform(df['clean_review'])
    y = df['sentiment']

    # Split the data into training and test sets (80% train, 20% test)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    return X_train, X_test, y_train, y_test, vectorizer

# Main function to execute the full workflow
def main():
    # Load and preprocess the dataset
    url = "https://docs.google.com/spreadsheets/d/1-4CMrIp98PsaNBZ5ioy86Arz2eACmvXPWMl7xizW7Q0/export?format=csv"
    df = load_and_preprocess_data(url)

    # Visualize the sentiment distribution
    plot_sentiment_distribution(df)

    # Generate word clouds for positive and negative reviews
    generate_wordclouds(df)

    # Prepare features and split data
    X_train, X_test, y_train, y_test, vectorizer = create_features_and_split(df)

    # Train and evaluate the model
    model, y_pred = train_and_evaluate_model(X_train, X_test, y_train, y_test)

if __name__ == "__main__":
    main()

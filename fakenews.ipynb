# Install necessary libraries
!pip install catboost gensim
!pip install --upgrade --force-reinstall numpy gensim

# Import necessary libraries
import numpy as np
import pandas as pd
import nltk
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import warnings
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import accuracy_score, classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
import matplotlib.pyplot as plt
import seaborn as sns

# Suppress warnings
warnings.filterwarnings('ignore')

# Download stopwords from NLTK
nltk.download('stopwords')

# Function for text preprocessing
def preprocess_text(text):
    """
    Preprocess the input text by converting it to lowercase, removing non-alphabetic characters,
    tokenizing, and removing stopwords.
    """
    text = text.lower()  # Convert text to lowercase
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters and digits
    tokens = word_tokenize(text)  # Tokenize the text
    stop_words = set(stopwords.words('english'))  # Set of English stopwords
    tokens = [token for token in tokens if token not in stop_words]  # Remove stopwords
    return ' '.join(tokens)

# Function to load and preprocess the dataset
def load_and_preprocess_data(file_path):
    """
    Load the dataset and apply text preprocessing.
    """
    data = pd.read_csv(file_path)
    data['processed_text'] = data['text'].apply(preprocess_text)
    return data

# Function to vectorize text data using TF-IDF
def vectorize_text(data, max_features=5000):
    """
    Convert text data into TF-IDF features.
    """
    tfidf_vectorizer = TfidfVectorizer(max_features=max_features)
    X = tfidf_vectorizer.fit_transform(data['processed_text'])
    return X, tfidf_vectorizer

# Function to train and evaluate multiple models
def train_and_evaluate_models(X_train, X_test, y_train, y_test, models):
    """
    Train and evaluate a list of models.
    """
    results = {}
    for name, model in models.items():
        model.fit(X_train, y_train)  # Train the model
        y_pred = model.predict(X_test)  # Make predictions

        accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy
        report = classification_report(y_test, y_pred)  # Generate classification report

        # Store the results
        results[name] = {'accuracy': accuracy, 'classification_report': report}
        print(f"\n--- {name} Results ---")
        print(f"Accuracy: {accuracy}")
        print("Classification Report:")
        print(report)

    return results

# Function to visualize predicted label distribution
def visualize_label_distribution(y_pred, model_name):
    """
    Visualize the distribution of predicted labels.
    """
    plt.figure(figsize=(6, 4))
    sns.countplot(x=y_pred)
    plt.title(f"Distribution of Predicted Labels - {model_name}")
    plt.xlabel("Predicted Label")
    plt.ylabel("Count")
    plt.show()

# Function to prepare and save submission file
def prepare_submission(test_data, model, tfidf_vectorizer, model_name):
    """
    Prepare and save the submission file with predicted labels for the test dataset.
    """
    test_data['processed_text'] = test_data['text'].apply(preprocess_text)
    X_test_tfidf = tfidf_vectorizer.transform(test_data['processed_text'])
    y_pred = model.predict(X_test_tfidf)

    submission = pd.DataFrame({
        'ID': test_data['ID'],
        'label': y_pred.ravel()
    })
    submission.to_csv(f"{model_name[:5]}_submission.csv", index=False)
    print(f"Submission file saved as {model_name[:5]}_submission.csv")

# Main function to execute the workflow
def main():
    # Load and preprocess data
    train_data = load_and_preprocess_data('depi_r_2_competition_1.csv')

    # Vectorize text using TF-IDF
    X, tfidf_vectorizer = vectorize_text(train_data)
    y = train_data['label']
    
    # Split the data into training and test sets
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)

    # Define the models to train
    models = {
        'Logistic Regression': LogisticRegression(),
        'Random Forest': RandomForestClassifier(),
        'Gradient Boosting': GradientBoostingClassifier(),
        'SVM': SVC(),
        'XGBoost': XGBClassifier(),
        'LightGBM': LGBMClassifier(),
        'CatBoost': CatBoostClassifier()
    }

    # Train and evaluate models
    model_results = train_and_evaluate_models(X_train, X_test, y_train, y_test, models)

    # Test data preparation and model submission
    test_data = pd.read_csv('x_test.csv')
    for name, model in models.items():
        # Train on the full dataset
        model.fit(X, y)
        prepare_submission(test_data, model, tfidf_vectorizer, name)
        visualize_label_distribution(model.predict(X_test), name)

if __name__ == "__main__":
    main()

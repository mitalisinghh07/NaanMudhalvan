# Install necessary libraries
!pip install transformers datasets scikit-learn --quiet

import pandas as pd
import numpy as np
import torch
from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix
from datasets import Dataset
import seaborn as sns
import matplotlib.pyplot as plt


class TextClassificationModel:
    def __init__(self, model_name="distilbert-base-uncased", max_length=128):
        """
        Initialize the model with tokenizer and the specified model name.
        """
        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)
        self.model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=2)
        self.max_length = max_length

    def load_data(self, file_path):
        """
        Load and preprocess the dataset.
        """
        df = pd.read_excel(file_path)
        df["input"] = df["text"] + " [SEP] " + df["reason"]
        df = df[["input", "label"]].dropna()
        df = df.sample(n=100, random_state=42)
        return df

    def tokenize_batch(self, batch):
        """
        Tokenize the input batch using the tokenizer.
        """
        return self.tokenizer(batch["input"], padding="max_length", truncation=True, max_length=self.max_length)

    def prepare_datasets(self, df):
        """
        Split the dataframe into training and testing datasets and tokenize them.
        """
        train_df, test_df = train_test_split(df, test_size=0.2, stratify=df["label"], random_state=42)
        train_dataset = Dataset.from_pandas(train_df)
        test_dataset = Dataset.from_pandas(test_df)

        train_dataset = train_dataset.map(self.tokenize_batch, batched=True)
        test_dataset = test_dataset.map(self.tokenize_batch, batched=True)

        train_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])
        test_dataset.set_format(type="torch", columns=["input_ids", "attention_mask", "label"])

        return train_dataset, test_dataset

    def create_trainer(self, train_dataset, test_dataset):
        """
        Set up the Trainer object for model training and evaluation.
        """
        training_args = TrainingArguments(
            output_dir="./results",
            num_train_epochs=3,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            logging_strategy="no",
            save_strategy="no",
            report_to=[],  # Disable WandB
        )

        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=test_dataset,
            compute_metrics=self.compute_metrics,
        )

        return trainer

    def compute_metrics(self, predictions):
        """
        Compute accuracy, precision, recall, and F1-score from model predictions.
        """
        labels = predictions.label_ids
        preds = np.argmax(predictions.predictions, axis=1)
        acc = accuracy_score(labels, preds)
        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')
        return {"accuracy": acc, "precision": precision, "recall": recall, "f1": f1}

    def plot_confusion_matrix(self, y_true, y_pred):
        """
        Plot confusion matrix using seaborn heatmap.
        """
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
        plt.title("Confusion Matrix")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.show()

    def train_and_evaluate(self, file_path):
        """
        Load data, prepare datasets, train the model, and evaluate the results.
        """
        # Load and preprocess data
        df = self.load_data(file_path)

        # Prepare datasets
        train_dataset, test_dataset = self.prepare_datasets(df)

        # Create and train the model
        trainer = self.create_trainer(train_dataset, test_dataset)
        trainer.train()

        # Evaluate the model
        predictions = trainer.predict(test_dataset)
        y_true = predictions.label_ids
        y_pred = np.argmax(predictions.predictions, axis=1)

        acc = accuracy_score(y_true, y_pred)
        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average="binary")

        print(f"\nüìä Evaluation Metrics:")
        print(f"Accuracy:  {acc:.4f}")
        print(f"Precision: {precision:.4f}")
        print(f"Recall:    {recall:.4f}")
        print(f"F1-score:  {f1:.4f}")

        # Plot confusion matrix
        self.plot_confusion_matrix(y_true, y_pred)

    def predict_alignment(self, text, reason):
        """
        Predict whether the feedback is aligned or not based on the text and reason.
        """
        input_text = text + " [SEP] " + reason
        inputs = self.tokenizer(input_text, return_tensors="pt", truncation=True, padding=True, max_length=self.max_length)

        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
            prediction = torch.argmax(probs, dim=1).item()
            confidence = probs[0][prediction].item()

        label = "Aligned ‚úÖ" if prediction == 1 else "Not Aligned ‚ùå"
        return label, confidence


# Usage example
def main():
    model = TextClassificationModel()

    # Train and evaluate the model
    file_path = "train.xlsx"  # Path to your dataset
    model.train_and_evaluate(file_path)

    # Example user input for alignment prediction
    feedback = "The course was too fast to follow."
    reason = "Course pace issue"

    # Get prediction for user input
    label, confidence = model.predict_alignment(feedback, reason)
    print(f"Prediction: {label}")
    print(f"Confidence: {confidence:.2f}")


if __name__ == "__main__":
    main()
